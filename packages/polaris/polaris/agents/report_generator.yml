version: 1
id: report_generator
kind: agent_pipeline
description: Generate detailed reports from Galaxy history analysis

start: fetch_histories

inputs:
  transcripts:
    type: transcript_stream
  context:
    type: object

state:
  histories:
    type: array
  selected_history:
    type: object
  contents:
    type: array
  dataset_details:
    type: array
  job_details:
    type: array
  job_parameters:
    type: array
  citations:
    type: array
  workflow_analysis:
    type: string
  report:
    type: string

nodes:
  # Step 1: Fetch available histories
  fetch_histories:
    type: executor
    run:
      op: api.call
      target: galaxy.histories.get
      input:
        limit: 20
    emit:
      state.histories: result
    next: select_history

  # Step 2: Use AI to select the appropriate history based on user request
  select_history:
    type: planner
    prompt: |
      You are helping a researcher generate a methods section for their publication.
      Select the history that best matches what the user wants to document.
      If the user mentioned a specific analysis or history name, select that one.
      Otherwise, select the most recently updated history with completed jobs.
    enum_from:
      state: histories
      field: name
    output_schema:
      type: object
      required: [history_name]
      properties:
        history_name:
          type: string
    emit:
      state.selected_history:
        name:
          $ref: result.history_name
    next: fetch_history_details

  # Step 3: Look up history ID and fetch history details
  fetch_history_details:
    type: executor
    run:
      op: api.call
      target: galaxy.histories.show.get
      input:
        history_id:
          $expr:
            op: lookup
            from:
              $ref: state.histories
            match:
              field: name
              equals:
                $ref: state.selected_history.name
            select: id
    emit:
      state.selected_history: result
    next: fetch_contents

  # Step 4: Fetch the history contents to get dataset list (filtered for ok, non-deleted, visible)
  fetch_contents:
    type: executor
    run:
      op: api.call
      target: galaxy.histories.show.contents.get
      input:
        history_id:
          $ref: state.selected_history.id
        v: dev
        limit: 30
        q:
          - state-eq
          - deleted
          - visible
        qv:
          - ok
          - false
          - true
    emit:
      state.contents: result
    next: fetch_dataset_details

  # Step 5: Loop through datasets to get details including creating_job and file info
  fetch_dataset_details:
    type: loop
    over:
      $ref: state.contents
    as: dataset
    concurrency: 5
    delay: 0.05
    on:
      warning: fetch_job_details  # Continue even if some datasets fail
    execute:
      op: api.call
      target: galaxy.datasets.show.get
      input:
        dataset_id:
          $ref: loop.dataset.id
    emit:
      state.dataset_details:
        $append:
          id:
            $ref: result.id
          name:
            $ref: result.name
          creating_job:
            $ref: result.creating_job
          file_ext:
            $ref: result.file_ext
          file_size:
            $ref: result.file_size
          genome_build:
            $ref: result.genome_build
    next: fetch_job_details

  # Step 6: Loop through unique jobs (deduplicated by creating_job) to fetch job info
  fetch_job_details:
    type: loop
    over:
      $expr:
        op: unique
        from:
          $ref: state.dataset_details
        by: creating_job
    as: ds
    concurrency: 5
    delay: 0.05
    on:
      warning: fetch_job_parameters  # Continue even if some jobs fail to fetch
    execute:
      op: api.call
      target: galaxy.jobs.show.get
      input:
        job_id:
          $ref: loop.ds.creating_job
    emit:
      state.job_details:
        $append:
          job_id:
            $ref: result.id
          tool_id:
            $ref: result.tool_id
          tool_version:
            $ref: result.tool_version
          state:
            $ref: result.state
          create_time:
            $ref: result.create_time
    next: fetch_job_parameters

  # Step 7: Loop through jobs to fetch tool parameters
  fetch_job_parameters:
    type: loop
    over:
      $ref: state.job_details
    as: job
    concurrency: 5
    delay: 0.05
    on:
      warning: fetch_citations  # Continue even if some parameters fail
    execute:
      op: api.call
      target: galaxy.jobs.show.parameters_display.get
      input:
        job_id:
          $ref: loop.job.job_id
    emit:
      state.job_parameters:
        $append:
          job_id:
            $ref: loop.job.job_id
          tool_id:
            $ref: loop.job.tool_id
          parameters:
            $ref: result
    next: fetch_citations

  # Step 8: Fetch citations for tools used in this history
  fetch_citations:
    type: executor
    run:
      op: api.call
      target: galaxy.histories.show.citations.get
      input:
        history_id:
          $ref: state.selected_history.id
    emit:
      state.citations: result
    next: analyze_workflow

  # Step 9: Use AI to understand the analysis workflow
  analyze_workflow:
    type: reasoning
    prompt: |
      Analyze this Galaxy history and create a structured workflow description.

      You have access to:
      - Dataset details with file formats (file_ext field) and sizes
      - Job details with tool IDs and versions
      - Tool parameters showing actual settings used

      Create a structured analysis:

      1. DATA INPUTS: List input files with their formats (e.g., "paired-end FASTQ files")
         Look at datasets with file_ext like fastq, fastqsanger, bam, vcf, tabular, etc.

      2. ANALYSIS STEPS: For each analysis tool (exclude "__" internal tools), describe:
         - Tool name and what it does
         - Key parameters from job_parameters (look for scientifically relevant settings)
         - Input/output file types when apparent

      3. OUTPUTS: Describe final output files and their formats

      Focus on the scientific workflow. Exclude internal Galaxy tools starting with "__".
      Only mention tools that are ACTUALLY in the job_details - do not hallucinate.
    input:
      history_name:
        $ref: state.selected_history.name
      dataset_details:
        $ref: state.dataset_details
      job_details:
        $ref: state.job_details
      job_parameters:
        $ref: state.job_parameters
    emit:
      state.workflow_analysis:
        $ref: result
    next: generate_methods

  # Step 10: Generate the detailed report
  generate_methods:
    type: reasoning
    prompt: |
      Write a detailed methods section for this Galaxy analysis (~400-500 words).

      Structure the report with these sections:

      ## Data Processing Pipeline
      Describe the overall workflow in narrative form, written in past tense.
      Mention input data types and the general analysis approach.

      ## Tools and Parameters
      For each analysis tool used, include:
      - Tool name and version (e.g., "RNA-STAR v2.7.11a")
      - Key parameters/settings that affect the analysis
      - Brief description of what the tool does in the workflow

      ## Data Formats
      Summarize input file formats and output file formats.

      Guidelines:
      - Write in past tense, third person (scientific style)
      - Be specific about tool versions and key parameters
      - Exclude internal Galaxy tools (__DATA_FETCH__, __SET_METADATA__, etc.)
      - Only mention tools that appear in the workflow_analysis
      - Do NOT hallucinate tool names or parameters
    input:
      history_name:
        $ref: state.selected_history.name
      workflow_analysis:
        $ref: state.workflow_analysis
      citations:
        $ref: state.citations
    emit:
      state.report:
        $ref: result
    next: done

  # Terminal node - return the generated report
  done:
    type: terminal
    output:
      selected_history:
        id:
          $ref: state.selected_history.id
        name:
          $ref: state.selected_history.name
      workflow_analysis:
        $ref: state.workflow_analysis
      report:
        $ref: state.report
